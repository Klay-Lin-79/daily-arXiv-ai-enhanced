<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文评估了不同参数规模（500M和2.2B）的SmolVLM2模型在盲人和低视力用户视频描述任务上的表现，提出了两个专门的可访问性评估框架，并测试了多种提示策略和移动设备部署方案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能生成高质量视频描述，但其高内存、计算和部署需求阻碍了实际应用，特别是对依赖详细上下文感知描述的盲人和低视力用户。

Method: 在两个多样化数据集（AVCaps户外和Charades室内）上评估SmolVLM2变体；引入两个新颖的BLV可访问性评估框架：多上下文BLV框架和导航辅助框架；系统评估四种提示设计策略；在智能手机上部署模型并评估FP32和INT8精度变体。

Result: 研究模型规模对可访问性描述质量的影响，评估了不同参数规模模型在资源受限移动设备上的实际性能约束。

Conclusion: 通过专门的评估框架和移动部署测试，为开发适合盲人和低视力用户使用的轻量级视频描述模型提供了重要参考。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: Depth Anything 3 (DA3) 是一个从任意数量视觉输入预测空间一致几何的模型，无需已知相机姿态。通过简化设计，使用单一Transformer骨干和深度射线预测目标，在教师-学生训练范式下达到与DA2相当的细节和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 追求最小化建模，探索单一普通Transformer作为骨干的可行性，以及单一深度射线预测目标替代复杂多任务学习的可能性。

Method: 使用单一普通Transformer（如DINO编码器）作为骨干，采用深度射线预测目标，通过教师-学生训练范式进行训练。

Result: 在新建的视觉几何基准测试中，DA3在所有任务上达到新SOTA，相机姿态精度平均提升44.3%，几何精度提升25.1%，在单目深度估计上超越DA2。

Conclusion: DA3证明了简单架构设计的有效性，在视觉几何任务上实现了最先进的性能，所有模型均在公共学术数据集上训练。

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [3] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出自一致性采样（SCS）方法，通过引入视觉扰动和轨迹重采样，解决多模态大语言模型在强化学习中因猜测正确答案而获得相同奖励的问题，显著提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试中，基于结果奖励的强化学习面临一个关键问题：即使推理链错误但猜对答案的轨迹也会获得与正确推理相同的奖励，这影响了模型的学习效果。

Method: 提出自一致性采样（SCS）：对每个问题引入小视觉扰动，对初始轨迹进行重复截断和重采样，通过轨迹间的一致性得分来降低不可靠轨迹在策略更新中的权重。

Result: 在Qwen2.5-VL-7B-Instruct模型上，将SCS集成到RLOO、GRPO和REINFORCE++系列方法中，在六个多模态基准测试中准确率最高提升7.7个百分点，计算开销可忽略不计。

Conclusion: SCS为多模态大语言模型的结果奖励强化学习提供了一个简单通用的解决方案，在多个模型上都取得了显著效果提升。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [4] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 该论文提出了一种编码器增强的因果解码器模型架构，在有限硬件条件下实现比因果变换器更高的压缩效率，并通过基于熵的训练方法提高了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于因果语言模型的语言压缩算法虽然高效，但准确估计语言熵在计算上不可行，限制了模型压缩效率和泛化能力的进一步提升。

Method: 引入了编码器增强的因果解码器模型架构，通过基于每词元熵估计的训练方法，使模型接近但不超出训练数据的熵值。

Result: 新架构在训练效率上表现更优，压缩效果更好，且基于熵训练的模型比仅最小化损失的模型具有更好的泛化性能。

Conclusion: 通过考虑语言熵约束来训练语言模型，可以同时提升压缩效率和泛化能力，为高效语言建模提供了新思路。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [5] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: SSR是一个新颖的LLM推理框架，通过将模型响应分解为可验证的子问题-子答案对，实现细粒度评估和精确优化，在多个推理基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗糙的自验证和自校正，限制了在复杂任务上的有效性，需要更精细的推理评估和优化方法。

Method: 将模型响应分解为可验证的子问题-子答案对，通过控制重解和自一致性检查进行步骤级置信度估计，精确定位不可靠步骤并迭代优化。

Result: 在五个推理基准测试和三个LLM上的实证结果表明，SSR始终优于最先进的迭代自优化基线方法。

Conclusion: SSR不仅带来性能提升，还为评估和理解LLM内部推理过程提供了原则性的黑盒方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [6] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开放的30亿参数语言模型家族，使用公开数据和代码库训练，在AMD MI300X GPU上开发，尽管预训练token较少但在完全开放模型中达到SOTA水平，并发布了支持128K上下文和数学推理的变体。


<details>
  <summary>Details</summary>
Motivation: 解决当前高性能语言模型多为闭源或部分开源的问题，推动语言模型研究的透明度和可复现性。

Method: 使用AMD MI300X GPU进行大规模预训练、通用指令微调以及人类偏好对齐，并开发了Instella-Long（支持128K上下文）和Instella-Math（数学推理增强）两个专门变体。

Result: Instella在完全开放模型中达到最先进水平，与同类规模的领先开放权重模型具有竞争力，尽管使用的预训练token数量显著少于许多同类模型。

Conclusion: Instella为社区提供了一个透明、高性能且多功能的替代方案，推动了开放和可复现语言模型研究的目标。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [7] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出生成对抗蒸馏（GAD）方法，通过将学生LLM作为生成器、训练判别器区分学生与教师模型的响应，实现黑盒蒸馏，在LMSYS-Chat评估中使Qwen2.5-14B-Instruct学生模型达到与GPT-5-Chat教师模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒蒸馏问题，即仅从专有教师模型的文本输出学习，而不访问其内部logits或参数，实现更有效的知识迁移。

Method: 采用生成对抗蒸馏框架，将学生LLM作为生成器，训练判别器区分学生与教师模型的响应，形成极小极大博弈，判别器作为在线奖励模型与学生共同进化。

Result: GAD在实验中持续超越常用的序列级知识蒸馏方法，Qwen2.5-14B-Instruct学生模型在LMSYS-Chat自动评估中达到与GPT-5-Chat教师模型相当的水平。

Conclusion: GAD被确立为黑盒LLM蒸馏的一个有前景且有效的范式。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [8] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种仅权重的后训练量化方法，通过成对旋转量化和通道级缩放来减少大语言模型中的异常值影响，在推理任务上比AWQ平均提升2.4%准确率，且运行时开销小于10%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型权重和激活中的异常值会导致量化误差和精度下降，特别是在推理任务中误差会在长思维链中累积。现有方法要么无法充分抑制异常值，要么在推理时引入显著开销。

Method: 结合硬件高效的独立Givens旋转和通道级缩放，平衡不同通道的幅度并缩小每个量化组内的动态范围，同时协同设计推理内核以充分利用GPU并行性。

Result: 在推理任务上比AWQ平均提升2.4%准确率，运行时开销小于10%。

Conclusion: 为更高效和准确部署推理大语言模型铺平了道路。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 本文提出了两个新的参数化多臂老虎机算法家族，通过离线数据学习近最优算法，在满足凹性强度等额外性质时获得更强的性能保证，同时保证在良好实例上的最优臂识别和在恶劣实例上的最坏情况保证。


<details>
  <summary>Details</summary>
Motivation: 改进多臂老虎机问题用于在不确定性下分配努力，如投资新技术研究、临床试验和超参数选择。现有算法的最坏情况保证较为悲观，存在Ω(k)和Ω(√k)的下界。本文旨在通过参数化算法家族和数据依赖分析获得更强的性能保证。

Method: 提出了两个参数化算法家族：第一个包含先前工作中的最优随机算法，在奖励曲线满足凹性强度相关性质时可获得更强的k依赖保证；第二个家族保证在良好实例上识别最优臂，在恶劣实例上回退到最坏情况保证。采用统计学习视角，通过离线数据学习近最优算法。

Result: 展示了从第一个家族适当选择的算法在满足额外性质时可获得更强的k依赖保证；第二个家族实现了数据依赖的更强保证，无需验证假设是否满足。

Conclusion: 通过参数化算法家族和统计学习视角，本文在改进多臂老虎机问题上获得了比现有工作更强的数据依赖性能保证，同时避免了假设验证的需求。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>
